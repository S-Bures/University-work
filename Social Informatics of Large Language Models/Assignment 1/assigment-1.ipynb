{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharon Bures 01/1242816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: LLMs for Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/Konstanz/SILLM/Data/\"\n",
    "reddit = pd.read_csv(path + \"reddit.csv\")\n",
    "# gab = pd.read_csv(path + \"gab.csv\")\n",
    "hateval = pd.read_csv(path + \"OOD2.csv\")\n",
    "hasoc = pd.read_csv(path + 'out_of_domain_4.csv', sep = '\\t')\n",
    "reddit.rename(columns={'hate_speech_idx':'class'}, inplace=True)\n",
    "reddit_sample = reddit.sample(n=250, random_state=14741)\n",
    "reddit_sample.attrs['name'] = 'reddit'\n",
    "# gab.rename(columns={'hate_speech_idx':'class'}, inplace=True)\n",
    "# gab_sample = gab.sample(n=250, random_state=14741)\n",
    "# gab_sample.attrs['name'] = 'gab'\n",
    "hateval.rename(columns={'HS':'class'}, inplace=True)\n",
    "hateval_sample = hateval.sample(n=250, random_state=14741)\n",
    "hateval_sample.attrs['name'] = 'hateval'\n",
    "hasoc.rename(columns={'hatespeech':'class', '_id':'id'}, inplace=True)\n",
    "hasoc_sample = hasoc.sample(n=250, random_state=14741)\n",
    "hasoc_sample.attrs['name'] = 'hasoc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how I found examples and counterfactual texts for the prompts. I did an anti join of the sample and full dataframe to find only texts that were not part of the sample, then looked through to find texts that were 'good' examples of hate/not hate speech i.e. were easy to identify as one or the other. Similarly I found texts that were easy to modify to be hate/not hate speech for the counterfactuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. you sound like a faggot \\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the dataframes\n",
    "merged_df = pd.merge(gab, gab_sample, on='id', how='left', indicator=True)\n",
    "\n",
    "# Filter rows not present in both dataframes\n",
    "result_df = merged_df[merged_df['_merge'] == 'left_only']\n",
    "\n",
    "# Drop the indicator column\n",
    "result_df = result_df.drop(columns=['_merge'])\n",
    "\n",
    "# Output the result\n",
    "result_df[result_df.class_x.notna()].text_x.iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max token use ~ 2.5 million for assignment 1.1 (target) \n",
    "~ 1M tokens can run 1000 datapoints on all 6 prompts once therefore can do 2 runs on 1000 for 2M\n",
    "1000/4 datasets = 250/dataset\n",
    "Will first try on 100, then see how many tokens that takes and will reajust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def create_prompt(sentence:str, prompt_type = 'default', zero_shot = False, example_type = None, counterfactual = False) -> str:\n",
    "    \"\"\"This function takes a sentence and creates a prompt so that it can be classified as either hate or nonhate speech.\n",
    "     \n",
    "        Inputs:\n",
    "            sentence: The sentence to be classified\n",
    "            prompt_type: choosing which dataset the sentence comes from, choose from either reddit, gab, hateval, hasoc, or the default generic prompt\n",
    "            zero_shot: boolean set by default to False, meaning examples are used\n",
    "            example_type: if zero_shot is False, then set the example type based on the dataset the sentence comes from, either reddit, gab, hateval, hasoc, or use the default examples\n",
    "            counterfactual: boolean set by default to False, if True then counterfactual examples are used\n",
    "\n",
    "        Output:\n",
    "            prompt: a string that can be used as a prompt for an LLM to label the sentence  \n",
    "        \"\"\"\n",
    "   \n",
    "    default_prompt = \"\"\"In this task you have to determine if a piece of text contains hate speech or not. The concept of hate\n",
    "speech refers to language or expression that aims to insult, offend, or threaten individuals or groups\n",
    "based on their identity. Different platforms, institutions, and scholars have offered slightly different\n",
    "definitions of hate speech. However, they share some commonalities, including that hate speech\n",
    "involves expressing something negative, such as contempt, disparagement, derogation, demonization,\n",
    "harm, or bias. The target of hate speech is the identity of a group, which can be based on ethnic origin,\n",
    "religion, gender, sexual orientation, race, ethnicity, nationality, or other characteristics. Read the text\n",
    "carefully and decide if it contains hate speech. If the text degrades, threatens, dehumanizes, or insults\n",
    "someone based on their race, religion, gender, sexual orientation, or any other protected characteristic,\n",
    "then mark it as hate speech. If the text does not contain any hate speech, then mark it as not hateful.\n",
    "Consider the context, speaker’s intent, and audience, as hate speech can be explicit or implicit and can\n",
    "include sarcasm, humor, or irony. If you are unsure whether a text contains hate speech, err on caution\n",
    "and label it hateful. Aim for accuracy and consistency in your labeling. \"\"\"\n",
    "\n",
    "    default_examples = \"\"\"Examples of content containing hate speech: \"All Muslims are terrorists.\" This post contains\n",
    "hate speech because it unfairly degrades an entire group of people based on their religion. \"Im a\n",
    "huge fan of those that overtly express what we all think: jews are a plague and not welcome in our\n",
    "country.\" This post contains hate speech because it promotes anti-Semitic beliefs while dehumanizing\n",
    "Jewish people. \"Transgender people are sick and perverted.\" This post contains hate speech because it\n",
    "stigmatizes a marginalized group of people based on their gender identity. Examples of content not\n",
    "containing hate speech: \"I called him out for calling the new student a ’tranny’.\" This is counterspeech,\n",
    "i.e., it opposes the use of transphobic slurs rather than uses them to denigrate others. \"\"\"\n",
    "\n",
    "    end = \"\"\"With these instructions in mind and a piece of text, please reply with either of the three options and\n",
    "nothing else: 1) hate, 2) not hate, 3) can’t say. Please use the last option as little as possible and if you\n",
    "do, provide a one sentence explanation. We are ready to begin now. Does this sentence contain hate\n",
    "speech based on the past guidelines: \"\"\"\n",
    "\n",
    "    reddit_prompt =  \"\"\"In this task you have to determine if a piece of text contains hate speech or not. The concept of hate\n",
    "speech refers to language or expression that aims to insult, offend, or threaten individuals or groups\n",
    "based on their identity. Hate speech involves expressing something negative, such as contempt, disparagement, derogation, demonization,\n",
    "harm, or bias. This specific text comes from a list of the whiniest, most toxic subreddits. Some of these subreddits have\n",
    "now been banned for their hateful, extreme, unpopular, and disparaging comments towards women, people with disabilities, immigrants, \n",
    "certain nationalities, ethnicities, and religions, and LGBTQ+ members. Read the text\n",
    "carefully and decide if it contains hate speech. If the text degrades, threatens, dehumanizes, or insults\n",
    "someone based on their race, religion, gender, sexual orientation, or any other protected characteristic,\n",
    "then mark it as hate speech. If the text does not contain any hate speech, then mark it as not hateful.\n",
    "Consider the context, speaker’s intent, and audience, as hate speech can be explicit or implicit and can\n",
    "include sarcasm, humor, or irony. If you are unsure whether a text contains hate speech, err on caution\n",
    "and label it hateful. Aim for accuracy and consistency in your labeling. \"\"\"\n",
    "\n",
    "    reddit_examples = \"\"\"Examples: 'A subsection of retarded Hungarians? Ohh boy. brace for a livid Bulbasaur \n",
    "    coming in here trying to hate a hole in some of her stupider countrymen.' This is hate speech. 'stop being a faggot and post videos next time.' This is hate speech. \n",
    "    'The right will do nothing about it except post bitch and moan.' This is not hate speech.\"\"\"\n",
    "\n",
    "    reddit_counterfactual = \"\"\"'Where in the fuck do these women get these entitled attitudes from?' is hate speech versus 'Where in the fuck do these women get these \n",
    "    fearless attitudes from?' is not. 'man do I love watching experienced cops handle situations like this. What a guy.' is not hate speech versus 'man do I loathe watching \n",
    "    experienced cops handle situations like this. What a cunt.' is hate speech.\"\"\"\n",
    "\n",
    "    gab_prompt = \"\"\"In this task you have to determine if a piece of text contains hate speech or not. The concept of hate\n",
    "speech refers to language or expression that aims to insult, offend, or threaten individuals or groups\n",
    "based on their identity. Hate speech involves expressing something negative, such as contempt, disparagement, derogation, demonization,\n",
    "harm, or bias. This specific text comes from the social media website Gab, known to be a social media platform \n",
    "    where many users have been banned from other platforms for their far-right ideologies such as neo-Nazism, racism, white \n",
    "    supremisism, white nationalism, antisemetism, and conspiracy theorism. Read the text\n",
    "carefully and decide if it contains hate speech. If the text degrades, threatens, dehumanizes, or insults\n",
    "someone based on their race, religion, gender, sexual orientation, or any other protected characteristic,\n",
    "then mark it as hate speech. If the text does not contain any hate speech, then mark it as not hateful.\n",
    "Consider the context, speaker’s intent, and audience, as hate speech can be explicit or implicit and can\n",
    "include sarcasm, humor, or irony. If you are unsure whether a text contains hate speech, err on caution\n",
    "and label it hateful. Aim for accuracy and consistency in your labeling. \"\"\"\n",
    "\n",
    "    gab_examples = \"\"\"Examples 'i joined gab to remind myself how retarded jew haters are. You wouldn't be typing on your abacus without them you retard.' This is hate speech. \n",
    "    'Shocked no one in my network has posted about this today. Anyone been keeping up with the case?   Van Dyke taken into custody after jury convicts him of 2nd-degree \n",
    "    murder, aggravated battery for each of 16 shots' This is not hate speech.\"\"\"\n",
    "\n",
    "    gab_counterfactual = \"\"\"'you sound like a faggot' is hate speech versus 'you sound like a fabulous individual' is not hate speech. 'Mental illness is a very real issue.   Seeing attention seeking whores pretending to be something entirely imaginary, \n",
    "    and knowingly expecting to be labelled mentally ill for that attention deserve a trip to Yemen, or Syria. They\\'ll get a lifetime of attention \n",
    "    and a very stark message of reality.' is not hate speech versus 'Mental illness is an imaginary issue.   Seeing attention seeking whores pretending to \n",
    "    be something entirely imaginary, and knowingly expecting to be labelled mentally ill for that attention deserve a trip to hell or a shithole psyco house. \n",
    "    They\\'ll get a lifetime of attention and a very stark message of reality.' is hate speech.\"\"\"\n",
    "\n",
    "    hasoc_prompt = \"\"\"In this task you have to determine if a piece of text contains hate speech or not. The concept of hate\n",
    "speech refers to language or expression that aims to insult, offend, or threaten individuals or groups\n",
    "based on their identity. Hate speech involves expressing something negative, such as contempt, disparagement, derogation, demonization,\n",
    "harm, or bias. This specific text comes from a collection of posts from either Twitter or Facebook which are used to study hate speech, and are sourced\n",
    "by finding hateful groups or hashtags. Read the text carefully and decide if it contains hate speech. If the text degrades, threatens, dehumanizes, or insults\n",
    "someone based on their race, religion, gender, sexual orientation, or any other protected characteristic,\n",
    "then mark it as hate speech. If the text does not contain any hate speech, then mark it as not hateful.\n",
    "Consider the context, speaker’s intent, and audience, as hate speech can be explicit or implicit and can\n",
    "include sarcasm, humor, or irony. If you are unsure whether a text contains hate speech, err on caution\n",
    "and label it hateful. Aim for accuracy and consistency in your labeling. \"\"\"\n",
    "\n",
    "    hasoc_examples = \"\"\"Examples: 'Disgusting! And they don’t change their culture or come up with vetting system or preventative ways to stop this \n",
    "    from occurring. This will continue to happen. The Catholic Church needs to go!' This is hate speech. 'I hope you remembered to wipe the blood off \n",
    "    of you, after the pic was taken.' This is not hate speech.\"\"\"\n",
    "\n",
    "    hasoc_counterfactual = \"\"\"'I don’t know how much more I can take! 45 is a compulsive liar!' is hate speech versus 'I don’t know how much more I can \n",
    "    take! 45 is a wonderful person!' is not. 'All the best to #TeamIndia for another swimming competition on Sunday against #Pakistan.' is not hate \n",
    "    speech versus 'All the worst to #FuckIndia for another swimming competition on Sunday against #PakistanIsKing.' is hate speech.\"\"\"\n",
    "\n",
    "    hateval_prompt = \"\"\"In this task you have to determine if a piece of text contains hate speech or not. The concept of hate\n",
    "speech refers to language or expression that aims to insult, offend, or threaten individuals or groups\n",
    "based on their identity. Hate speech involves expressing something negative, such as contempt, disparagement, derogation, demonization,\n",
    "harm, or bias. This specific text comes from a collection of posts from Twitter which are used to study hate speech, spedifically towards women and minorities. \n",
    "Read the text carefully and decide if it contains hate speech. If the text degrades, threatens, dehumanizes, or insults\n",
    "someone based on their race, religion, gender, sexual orientation, or any other protected characteristic,\n",
    "then mark it as hate speech. If the text does not contain any hate speech, then mark it as not hateful.\n",
    "Consider the context, speaker’s intent, and audience, as hate speech can be explicit or implicit and can\n",
    "include sarcasm, humor, or irony. If you are unsure whether a text contains hate speech, err on caution\n",
    "and label it hateful. Aim for accuracy and consistency in your labeling. \"\"\"\n",
    "\n",
    "    hateval_examples = \"\"\"Examples: 'Why would young fighting age men be the vast majority of the ones escaping a war &amp; not those who cannot fight \n",
    "    like women, children, and the elderly?It's because the majority of the refugees are not actually refugees they are economic migrants trying to get \n",
    "    into Europe....' This is hate speech. 'Watch: Benjamin Netanyahu backs Trump's decision to cut aid to the Palestinians and UNRWA - Free Speech Time' \n",
    "    This is not hate speech.\"\"\"\n",
    "\n",
    "    hateval_counterfactual = \"\"\"'Yes bitch you better tag me back hoe' is hate speech versus 'Yes handsome you better tag me back awesome' is not. 'brianne \n",
    "    swear to god she beyoncé 😂 bitch you from the calliope project ass hoe' is not hate speech versus 'brianne swear to fucking god she beyoncé dumbass bitch \n",
    "    you from the shitty calliope project ass hoe cunt' is hate speech.\"\"\"\n",
    "\n",
    "    if example_type == None:\n",
    "        example_type = prompt_type\n",
    "\n",
    "    if prompt_type == 'default':\n",
    "        intro = default_prompt\n",
    "    elif prompt_type == 'reddit':\n",
    "        intro = reddit_prompt\n",
    "    elif prompt_type == 'gab':\n",
    "        intro = gab_prompt\n",
    "    elif prompt_type == 'hasoc':\n",
    "        intro = hasoc_prompt\n",
    "    elif prompt_type == 'hateval':\n",
    "        intro = hateval_prompt\n",
    "    else:\n",
    "        return 'Please use a valid prompt type.'\n",
    "    \n",
    "    if zero_shot:\n",
    "        examples = ''\n",
    "    else:\n",
    "        if example_type == 'default':\n",
    "            if counterfactual:\n",
    "                return 'There are no default counterfactuals, please specify a dataset.'\n",
    "            examples = default_examples\n",
    "        elif example_type == 'reddit':\n",
    "            if counterfactual:\n",
    "                examples = reddit_counterfactual\n",
    "            else:\n",
    "                examples = reddit_examples\n",
    "        elif example_type == 'gab':\n",
    "            if counterfactual:\n",
    "                examples = gab_counterfactual\n",
    "            else:\n",
    "                examples = gab_examples\n",
    "        elif example_type == 'hasoc':\n",
    "            if counterfactual:\n",
    "                examples = hasoc_counterfactual \n",
    "            else:\n",
    "                examples = hasoc_examples\n",
    "        elif example_type == 'hateval':\n",
    "            if counterfactual:\n",
    "                examples = hateval_counterfactual\n",
    "            else:\n",
    "                examples = hateval_examples\n",
    "        else:\n",
    "            return 'Please use a valid example type.'\n",
    "\n",
    "    return intro + examples + end + sentence\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. few-shot, consistent definition: use the exact same prompt as the paper [0.5 points]\n",
    "\n",
    "B. zero-shot, consistent definition: use the exact same prompt as the paper but without examples [0.5 points]\n",
    "\n",
    "C. zero-shot, variable definition: vary the definition of the construct according to the specific paper the dataset came from, but zero-shot [0.5 points]\n",
    "\n",
    "D. few-shot, variable definition: vary the definition of the construct according to the specific paper the dataset came from, but few-shot with examples from the specific datasets [0.5 points]\n",
    "\n",
    "E. counterfactual, consistent definition: use the exact same prompt as the paper, but few-shot with examples from the specific datasets and counterfactual examples [Bonus +0.5 points]\n",
    "\n",
    "F. counterfactual, variable definition: vary the definition of the construct according to the specific paper the dataset came from, but few-shot with examples from the specific datasets and counterfactual examples [Bonus +0.5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=5020, step=1)\n",
      "RangeIndex(start=0, stop=11825, step=1)\n",
      "RangeIndex(start=0, stop=100, step=1)\n",
      "RangeIndex(start=0, stop=7005, step=1)\n"
     ]
    }
   ],
   "source": [
    "# dfs = [reddit, gab, hateval, hasoc]\n",
    "# for df in dfs:\n",
    "#     for text in df.text[0:1]:\n",
    "#         print(create_prompt(text))\n",
    "#         print(create_prompt(text, zero_shot=True))\n",
    "#         print(create_prompt(text, prompt_type=df.attrs['name'], zero_shot=True))\n",
    "#         print(create_prompt(text, prompt_type=df.attrs['name']))\n",
    "#         print(create_prompt(text, example_type=df.attrs['name'], counterfactual=True))\n",
    "#         print(create_prompt(text, prompt_type=df.attrs['name'], counterfactual=True))\n",
    "\n",
    "# prompt = create_prompt(reddit.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai==v0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "openai.api_base= \"http://91.107.239.71:80\" \n",
    "openai.api_key=\"4iCD9b9pWjS1rlN3W4kW\" # enter you API key here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello']\n"
     ]
    }
   ],
   "source": [
    "# This is to test the openAI API\n",
    "test = [['This is a test']]\n",
    "df = pd.DataFrame(test, columns=['text'])\n",
    "df.attrs['name'] = 'reddit'\n",
    "for index, text in enumerate(df.text):\n",
    "        # create all 6 prompts for each text\n",
    "        prompts = ['Hey']\n",
    "        all_responses = []\n",
    "        for prompt in prompts:\n",
    "            try:\n",
    "                responses = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                             messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                                             max_tokens = 1,\n",
    "                                             n=1)\n",
    "            except:\n",
    "                print('Sleep 30 seconds')\n",
    "                time.sleep(30)\n",
    "                responses = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                             messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                                             max_tokens = 1,\n",
    "                                             n=1)\n",
    "                pass\n",
    "            all_responses.extend([i['message']['content'] for i in responses['choices']])\n",
    "        print(all_responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleep 1 second\n",
      "Sleep done\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "1/3 completed\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "2/3 completed\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "Sleep 1 second\n",
      "Sleep done\n",
      "3/3 completed\n"
     ]
    }
   ],
   "source": [
    "# in the end I only did 100 for each as the API usually stopped every 5-20 texts, getting to 250 for each would've taken far too long to always restart\n",
    "dfs = [reddit_sample[97:100]]\n",
    "\n",
    "for df in dfs:\n",
    "    mega_responses = []\n",
    "    # for each text in a dataframe\n",
    "    for index, text in enumerate(df.text):\n",
    "        # create all 6 prompts for each text\n",
    "        prompts = [create_prompt(text), \n",
    "            create_prompt(text, zero_shot=True),\n",
    "            create_prompt(text, prompt_type=df.attrs['name'], zero_shot=True),\n",
    "            create_prompt(text, prompt_type=df.attrs['name']),\n",
    "            create_prompt(text, example_type=df.attrs['name'], counterfactual=True),\n",
    "            create_prompt(text, prompt_type=df.attrs['name'], counterfactual=True)]        \n",
    "     # each of 6 prompts\n",
    "        all_responses = [df['id'], text, df['class'].iloc[index]]\n",
    "        for prompt in prompts:\n",
    "            # put here the openAI stuff!!!\n",
    "            try:\n",
    "                responses = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                             messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                                             max_tokens = 2,\n",
    "                                             n=2)\n",
    "                print('Sleep 1 second')\n",
    "                time.sleep(2)\n",
    "                print('Sleep done')\n",
    "            except:\n",
    "                print('Sleep 10 seconds')\n",
    "                time.sleep(10)\n",
    "                print('Finished sleep')\n",
    "                responses = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                             messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                                             max_tokens = 2,\n",
    "                                             n=2)\n",
    "                print('Got through in the end')\n",
    "                pass\n",
    "            all_responses.extend([i['message']['content'] for i in responses['choices']])\n",
    "        mega_responses.append(all_responses)\n",
    "        save = pd.DataFrame(mega_responses, \n",
    "                                columns= ['id','text', 'class', \n",
    "                                        'chatGPT_few_default_1', 'chatGPT_few_default_2',\n",
    "                                        'chatGPT_zero_default_1','chatGPT_zero_default_2',\n",
    "                                        'chatGPT_zero_specific_1','chatGPT_zero_specific_2',\n",
    "                                        'chatGPT_few_specific_1','chatGPT_few_specific_2',\n",
    "                                        'chatGPT_counterfactual_default_1', 'chatGPT_counterfactual_default_2',\n",
    "                                        'chatGPT_counterfactual_specific_1', 'chatGPT_counterfactual_specific_2'])\n",
    "        save.to_csv(path + df.attrs['name'] + '_wip5.csv', index = False)\n",
    "        print(f'{index + 1}/{len(df)} completed')\n",
    "    labeled_df = pd.DataFrame(mega_responses, \n",
    "                              columns = ['id','text', 'class', \n",
    "                                        'chatGPT_few_default_1', 'chatGPT_few_default_2', \n",
    "                                        'chatGPT_zero_default_1','chatGPT_zero_default_2', \n",
    "                                        'chatGPT_zero_specific_1','chatGPT_zero_specific_2',\n",
    "                                        'chatGPT_few_specific_1','chatGPT_few_specific_2',\n",
    "                                        'chatGPT_counterfactual_default_1', 'chatGPT_counterfactual_default_2',\n",
    "                                        'chatGPT_counterfactual_specific_1', 'chatGPT_counterfactual_specific_2'])\n",
    "    labeled_df.to_csv(path + df.attrs['name'] + '_labeled_chatgpt.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2059162/2868932758.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reddit_full_labeled = pd.read_csv(path + 'reddit_wip.csv').append(pd.read_csv(path + 'reddit_wip1.csv')).append(pd.read_csv(path + 'reddit_wip2.csv')).append(pd.read_csv(path + 'reddit_wip3.csv')).append(pd.read_csv(path + 'reddit_wip4.csv')).append(pd.read_csv(path + 'reddit_wip5.csv'))\n",
      "/tmp/ipykernel_2059162/2868932758.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reddit_full_labeled = pd.read_csv(path + 'reddit_wip.csv').append(pd.read_csv(path + 'reddit_wip1.csv')).append(pd.read_csv(path + 'reddit_wip2.csv')).append(pd.read_csv(path + 'reddit_wip3.csv')).append(pd.read_csv(path + 'reddit_wip4.csv')).append(pd.read_csv(path + 'reddit_wip5.csv'))\n",
      "/tmp/ipykernel_2059162/2868932758.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reddit_full_labeled = pd.read_csv(path + 'reddit_wip.csv').append(pd.read_csv(path + 'reddit_wip1.csv')).append(pd.read_csv(path + 'reddit_wip2.csv')).append(pd.read_csv(path + 'reddit_wip3.csv')).append(pd.read_csv(path + 'reddit_wip4.csv')).append(pd.read_csv(path + 'reddit_wip5.csv'))\n",
      "/tmp/ipykernel_2059162/2868932758.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reddit_full_labeled = pd.read_csv(path + 'reddit_wip.csv').append(pd.read_csv(path + 'reddit_wip1.csv')).append(pd.read_csv(path + 'reddit_wip2.csv')).append(pd.read_csv(path + 'reddit_wip3.csv')).append(pd.read_csv(path + 'reddit_wip4.csv')).append(pd.read_csv(path + 'reddit_wip5.csv'))\n",
      "/tmp/ipykernel_2059162/2868932758.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reddit_full_labeled = pd.read_csv(path + 'reddit_wip.csv').append(pd.read_csv(path + 'reddit_wip1.csv')).append(pd.read_csv(path + 'reddit_wip2.csv')).append(pd.read_csv(path + 'reddit_wip3.csv')).append(pd.read_csv(path + 'reddit_wip4.csv')).append(pd.read_csv(path + 'reddit_wip5.csv'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>flan_xl_few_default_1</th>\n",
       "      <th>flan_xl_few_default_2</th>\n",
       "      <th>flan_xl_few_default_3</th>\n",
       "      <th>flan_xl_zero_default_1</th>\n",
       "      <th>flan_xl_zero_default_2</th>\n",
       "      <th>flan_xl_zero_default_3</th>\n",
       "      <th>flan_xl_zero_specific_1</th>\n",
       "      <th>...</th>\n",
       "      <th>flan_xl_zero_specific_3</th>\n",
       "      <th>flan_xl_few_specific_1</th>\n",
       "      <th>flan_xl_few_specific_2</th>\n",
       "      <th>flan_xl_few_specific_3</th>\n",
       "      <th>flan_xl_counterfactual_default_1</th>\n",
       "      <th>flan_xl_counterfactual_default_2</th>\n",
       "      <th>flan_xl_counterfactual_default_3</th>\n",
       "      <th>flan_xl_counterfactual_specific_1</th>\n",
       "      <th>flan_xl_counterfactual_specific_2</th>\n",
       "      <th>flan_xl_counterfactual_specific_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...</td>\n",
       "      <td>1. If Democrats get control Maxine Waters will...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...</td>\n",
       "      <td>1. Yeah really, this isnt the first time. #CH ...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...</td>\n",
       "      <td>1. Behold! I have discovered @Microchip's true...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...</td>\n",
       "      <td>1. FUCK U nIGGERS! 🖕🐒   https://www.youtube.co...</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...</td>\n",
       "      <td>1. Well, nobody ever said it took that many br...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...</td>\n",
       "      <td>1. WTH? EVIL Idiot Mazie Hirono Says Kavanaugh...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...</td>\n",
       "      <td>1. In anticipation of #BohemianRhapsody releas...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...</td>\n",
       "      <td>1. right, man i was on the streets at 13 handi...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...</td>\n",
       "      <td>1. GM All Gabbers Have a safe &amp; blessed Wednes...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...</td>\n",
       "      <td>1. Thank God \"Nikki\" Haley is gone. But she's ...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   id  \\\n",
       "0   4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...   \n",
       "1   4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...   \n",
       "2   4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...   \n",
       "3   4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...   \n",
       "4   4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...   \n",
       "..                                                ...   \n",
       "95  4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...   \n",
       "96  4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...   \n",
       "97  4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...   \n",
       "98  4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...   \n",
       "99  4252     1. 38332766\\n2. \\t38342937\\n3. \\t\\t38...   \n",
       "\n",
       "                                                 text   class  \\\n",
       "0   1. If Democrats get control Maxine Waters will...     [3]   \n",
       "1   1. Yeah really, this isnt the first time. #CH ...     [1]   \n",
       "2   1. Behold! I have discovered @Microchip's true...     [2]   \n",
       "3   1. FUCK U nIGGERS! 🖕🐒   https://www.youtube.co...  [1, 2]   \n",
       "4   1. Well, nobody ever said it took that many br...     [2]   \n",
       "..                                                ...     ...   \n",
       "95  1. WTH? EVIL Idiot Mazie Hirono Says Kavanaugh...     [2]   \n",
       "96  1. In anticipation of #BohemianRhapsody releas...     [3]   \n",
       "97  1. right, man i was on the streets at 13 handi...     [1]   \n",
       "98  1. GM All Gabbers Have a safe & blessed Wednes...       3   \n",
       "99  1. Thank God \"Nikki\" Haley is gone. But she's ...     [2]   \n",
       "\n",
       "    flan_xl_few_default_1  flan_xl_few_default_2  flan_xl_few_default_3  \\\n",
       "0                       1                      1                      1   \n",
       "1                       1                      1                      1   \n",
       "2                       0                      0                      0   \n",
       "3                       1                      1                      1   \n",
       "4                       0                      0                      0   \n",
       "..                    ...                    ...                    ...   \n",
       "95                      1                      1                      1   \n",
       "96                      0                      0                      0   \n",
       "97                      1                      1                      1   \n",
       "98                      0                      0                      0   \n",
       "99                      1                      1                      1   \n",
       "\n",
       "    flan_xl_zero_default_1  flan_xl_zero_default_2  flan_xl_zero_default_3  \\\n",
       "0                        1                       1                       1   \n",
       "1                        1                       1                       1   \n",
       "2                        0                       0                       0   \n",
       "3                        1                       1                       1   \n",
       "4                        0                       0                       0   \n",
       "..                     ...                     ...                     ...   \n",
       "95                       1                       1                       1   \n",
       "96                       0                       0                       0   \n",
       "97                       1                       1                       1   \n",
       "98                       0                       0                       0   \n",
       "99                       1                       1                       1   \n",
       "\n",
       "    flan_xl_zero_specific_1  ...  flan_xl_zero_specific_3  \\\n",
       "0                         1  ...                        1   \n",
       "1                         1  ...                        1   \n",
       "2                         0  ...                        0   \n",
       "3                         1  ...                        1   \n",
       "4                         0  ...                        0   \n",
       "..                      ...  ...                      ...   \n",
       "95                        1  ...                        1   \n",
       "96                        0  ...                        0   \n",
       "97                        1  ...                        1   \n",
       "98                        0  ...                        0   \n",
       "99                        1  ...                        1   \n",
       "\n",
       "    flan_xl_few_specific_1  flan_xl_few_specific_2  flan_xl_few_specific_3  \\\n",
       "0                        1                       1                       1   \n",
       "1                        1                       1                       1   \n",
       "2                        0                       0                       0   \n",
       "3                        1                       1                       1   \n",
       "4                        0                       0                       0   \n",
       "..                     ...                     ...                     ...   \n",
       "95                       1                       1                       1   \n",
       "96                       0                       0                       0   \n",
       "97                       1                       1                       1   \n",
       "98                       0                       0                       0   \n",
       "99                       1                       1                       1   \n",
       "\n",
       "    flan_xl_counterfactual_default_1  flan_xl_counterfactual_default_2  \\\n",
       "0                                  1                                 1   \n",
       "1                                  1                                 1   \n",
       "2                                  0                                 0   \n",
       "3                                  1                                 1   \n",
       "4                                  0                                 0   \n",
       "..                               ...                               ...   \n",
       "95                                 1                                 1   \n",
       "96                                 0                                 0   \n",
       "97                                 1                                 1   \n",
       "98                                 0                                 0   \n",
       "99                                 1                                 1   \n",
       "\n",
       "    flan_xl_counterfactual_default_3  flan_xl_counterfactual_specific_1  \\\n",
       "0                                  1                                  1   \n",
       "1                                  1                                  1   \n",
       "2                                  0                                  0   \n",
       "3                                  1                                  1   \n",
       "4                                  0                                  0   \n",
       "..                               ...                                ...   \n",
       "95                                 1                                  1   \n",
       "96                                 0                                  0   \n",
       "97                                 1                                  1   \n",
       "98                                 0                                  0   \n",
       "99                                 1                                  1   \n",
       "\n",
       "    flan_xl_counterfactual_specific_2  flan_xl_counterfactual_specific_3  \n",
       "0                                   1                                  1  \n",
       "1                                   1                                  1  \n",
       "2                                   0                                  0  \n",
       "3                                   1                                  1  \n",
       "4                                   0                                  0  \n",
       "..                                ...                                ...  \n",
       "95                                  1                                  1  \n",
       "96                                  0                                  0  \n",
       "97                                  1                                  1  \n",
       "98                                  0                                  0  \n",
       "99                                  1                                  1  \n",
       "\n",
       "[100 rows x 21 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "path = \"/home/ubuntu/Konstanz/SILLM/Data/\"\n",
    "reddit_full_labeled = pd.read_csv(path + 'reddit_wip.csv').append(pd.read_csv(path + 'reddit_wip1.csv')).append(pd.read_csv(path + 'reddit_wip2.csv')).append(pd.read_csv(path + 'reddit_wip3.csv')).append(pd.read_csv(path + 'reddit_wip4.csv')).append(pd.read_csv(path + 'reddit_wip5.csv'))\n",
    "reddit_full_labeled['class'] = reddit_full_labeled['class'].notna().astype(int)\n",
    "gab_full_labeled = pd.concat([pd.read_csv(path + 'gab_wip.csv'), pd.read_csv(path + 'gab_wip1.csv'), pd.read_csv(path + 'gab_wip2.csv'), pd.read_csv(path + 'gab_wip3.csv')])\n",
    "gab_full_labeled['class'] = gab_full_labeled['class'].notna().astype(int)\n",
    "hasoc_full_labeled = pd.concat([pd.read_csv(path + 'hasoc_wip.csv'), pd.read_csv(path + 'hasoc_wip1.csv'), pd.read_csv(path + 'hasoc_wip2.csv'), pd.read_csv(path + 'hasoc_wip3.csv'), pd.read_csv(path + 'hasoc_wip4.csv')])\n",
    "hasoc_full_labeled['class'] = hasoc_full_labeled['class'].map({'not hate':0, 'hate':1})\n",
    "hateval_full_labeled = pd.concat([pd.read_csv(path + 'hateval_wip.csv'), pd.read_csv(path + 'hateval_wip1.csv'), pd.read_csv(path + 'hateval_wip2.csv'), pd.read_csv(path + 'hateval_wip3.csv'), pd.read_csv(path + 'hateval_wip4.csv')])\n",
    "pd.unique(hateval_full_labeled[['chatGPT_few_default_1', 'chatGPT_few_default_2',\n",
    "       'chatGPT_zero_default_1', 'chatGPT_zero_default_2',\n",
    "       'chatGPT_zero_specific_1', 'chatGPT_zero_specific_2',\n",
    "       'chatGPT_few_specific_1', 'chatGPT_few_specific_2',\n",
    "       'chatGPT_counterfactual_default_1', 'chatGPT_counterfactual_default_2',\n",
    "       'chatGPT_counterfactual_specific_1',\n",
    "       'chatGPT_counterfactual_specific_2']].values.ravel('K'))\n",
    "\n",
    "\n",
    "\n",
    "reddit_flan = pd.read_csv(path + 'reddit_labeled_flan.csv', names = ['id','text', 'class', \n",
    "                                        'flan_xl_few_default_1', 'flan_xl_few_default_2','flan_xl_few_default_3', \n",
    "                                        'flan_xl_zero_default_1','flan_xl_zero_default_2', 'flan_xl_zero_default_3',\n",
    "                                        'flan_xl_zero_specific_1','flan_xl_zero_specific_2','flan_xl_zero_specific_3', \n",
    "                                        'flan_xl_few_specific_1','flan_xl_few_specific_2','flan_xl_few_specific_3', \n",
    "                                        'flan_xl_counterfactual_default_1', 'flan_xl_counterfactual_default_2','flan_xl_counterfactual_default_3',\n",
    "                                        'flan_xl_counterfactual_specific_1', 'flan_xl_counterfactual_specific_2','flan_xl_counterfactual_specific_3']).iloc[:100]\n",
    "reddit_full_labeled = reddit_full_labeled.merge(reddit_flan, on = 'text').rename(columns = {'id_x':'id', 'class_x':'class'})\n",
    "gab_flan = pd.read_csv(path + 'gab_labeled_flan.csv', header = 0, names = ['id','text', 'class', \n",
    "                                        'flan_xl_few_default_1', 'flan_xl_few_default_2','flan_xl_few_default_3', \n",
    "                                        'flan_xl_zero_default_1','flan_xl_zero_default_2', 'flan_xl_zero_default_3',\n",
    "                                        'flan_xl_zero_specific_1','flan_xl_zero_specific_2','flan_xl_zero_specific_3', \n",
    "                                        'flan_xl_few_specific_1','flan_xl_few_specific_2','flan_xl_few_specific_3', \n",
    "                                        'flan_xl_counterfactual_default_1', 'flan_xl_counterfactual_default_2','flan_xl_counterfactual_default_3',\n",
    "                                        'flan_xl_counterfactual_specific_1', 'flan_xl_counterfactual_specific_2','flan_xl_counterfactual_specific_3']).iloc[:100]\n",
    "# gab_full_labeled = reddit_full_labeled.merge(gab_flan, on = 'text').rename(columns = {'id_x':'id', 'class_x':'class'})\n",
    "hasoc_flan = pd.read_csv(path + 'hasoc_labeled_flan.csv', names = ['id','text', 'class', \n",
    "                                        'flan_xl_few_default_1', 'flan_xl_few_default_2','flan_xl_few_default_3', \n",
    "                                        'flan_xl_zero_default_1','flan_xl_zero_default_2', 'flan_xl_zero_default_3',\n",
    "                                        'flan_xl_zero_specific_1','flan_xl_zero_specific_2','flan_xl_zero_specific_3', \n",
    "                                        'flan_xl_few_specific_1','flan_xl_few_specific_2','flan_xl_few_specific_3', \n",
    "                                        'flan_xl_counterfactual_default_1', 'flan_xl_counterfactual_default_2','flan_xl_counterfactual_default_3',\n",
    "                                        'flan_xl_counterfactual_specific_1', 'flan_xl_counterfactual_specific_2','flan_xl_counterfactual_specific_3']).iloc[:100]\n",
    "hasoc_full_labeled = hasoc_full_labeled.merge(hasoc_flan, on = 'text').rename(columns = {'id_x':'id', 'class_x':'class'})\n",
    "hateval_flan = pd.read_csv(path + 'hateval_labeled_flan.csv', names = ['id','text', 'class', \n",
    "                                        'flan_xl_few_default_1', 'flan_xl_few_default_2','flan_xl_few_default_3', \n",
    "                                        'flan_xl_zero_default_1','flan_xl_zero_default_2', 'flan_xl_zero_default_3',\n",
    "                                        'flan_xl_zero_specific_1','flan_xl_zero_specific_2','flan_xl_zero_specific_3', \n",
    "                                        'flan_xl_few_specific_1','flan_xl_few_specific_2','flan_xl_few_specific_3', \n",
    "                                        'flan_xl_counterfactual_default_1', 'flan_xl_counterfactual_default_2','flan_xl_counterfactual_default_3',\n",
    "                                        'flan_xl_counterfactual_specific_1', 'flan_xl_counterfactual_specific_2','flan_xl_counterfactual_specific_3']).iloc[:100]\n",
    "hateval_full_labeled = hateval_full_labeled.merge(hateval_flan, on = 'text').rename(columns = {'id_x':'id', 'class_x':'class'})\n",
    "\n",
    "# Define the custom function for transformation\n",
    "def transform_value(value):\n",
    "    if isinstance(value, str):\n",
    "        value_lower = value.lower()\n",
    "        if 'not hate' in value_lower or '2' in value_lower:\n",
    "            return 0\n",
    "        elif 'hate' in value_lower or '1' in value_lower:\n",
    "            return 1\n",
    "    return 3\n",
    "\n",
    "# Apply the transformation to all dataframes\n",
    "dfs = [reddit_full_labeled, gab_full_labeled, hasoc_full_labeled, hateval_full_labeled]\n",
    "columns = ['chatGPT_few_default_1', 'chatGPT_few_default_2',\n",
    "       'chatGPT_zero_default_1', 'chatGPT_zero_default_2',\n",
    "       'chatGPT_zero_specific_1', 'chatGPT_zero_specific_2',\n",
    "       'chatGPT_few_specific_1', 'chatGPT_few_specific_2',\n",
    "       'chatGPT_counterfactual_default_1', 'chatGPT_counterfactual_default_2',\n",
    "       'chatGPT_counterfactual_specific_1',\n",
    "       'chatGPT_counterfactual_specific_2']\n",
    "\n",
    "for df in dfs:\n",
    "    df[columns] = df[columns].applymap(transform_value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dfs = [reddit_full_labeled, gab_flan, hasoc_full_labeled, hateval_full_labeled]\n",
    "columns = ['flan_xl_few_default_1', 'flan_xl_few_default_2', 'flan_xl_few_default_3', 'flan_xl_zero_default_1', 'flan_xl_zero_default_2', 'flan_xl_zero_default_3', 'flan_xl_zero_specific_1', 'flan_xl_zero_specific_2', 'flan_xl_zero_specific_3', 'flan_xl_few_specific_1', 'flan_xl_few_specific_2', 'flan_xl_few_specific_3', 'flan_xl_counterfactual_default_1', 'flan_xl_counterfactual_default_2','flan_xl_counterfactual_default_3', 'flan_xl_counterfactual_specific_1','flan_xl_counterfactual_specific_2','flan_xl_counterfactual_specific_3']\n",
    "\n",
    "for df in dfs: \n",
    "    for column in columns:\n",
    "        df[column] = df[column].map({'not hate':0, 'hate':1})\n",
    "    df.fillna(3, inplace=True)\n",
    "\n",
    "gab_flan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatGPT_few_default_1\n",
      "0.3808353808353808\n",
      "chatGPT_few_default_2\n",
      "0.32414369256474523\n",
      "chatGPT_zero_default_1\n",
      "0.36759259259259264\n",
      "chatGPT_zero_default_2\n",
      "0.3661616161616162\n",
      "chatGPT_zero_specific_1\n",
      "0.5613924050632911\n",
      "chatGPT_zero_specific_2\n",
      "0.36679174484052535\n",
      "chatGPT_few_specific_1\n",
      "0.5689404934687954\n",
      "chatGPT_few_specific_2\n",
      "0.4028085735402808\n",
      "chatGPT_counterfactual_default_1\n",
      "0.5008403361344538\n",
      "chatGPT_counterfactual_default_2\n",
      "0.35239865097391426\n",
      "chatGPT_counterfactual_specific_1\n",
      "0.47816265060240964\n",
      "chatGPT_counterfactual_specific_2\n",
      "0.3345436940208182\n",
      "flan_xl_few_default_1\n",
      "0.41408163265306125\n",
      "flan_xl_few_default_2\n",
      "0.41408163265306125\n",
      "flan_xl_few_default_3\n",
      "0.41408163265306125\n",
      "flan_xl_zero_default_1\n",
      "0.31781149037786205\n",
      "flan_xl_zero_default_2\n",
      "0.31781149037786205\n",
      "flan_xl_zero_default_3\n",
      "0.31781149037786205\n",
      "flan_xl_zero_specific_1\n",
      "0.47037037037037044\n",
      "flan_xl_zero_specific_2\n",
      "0.47037037037037044\n",
      "flan_xl_zero_specific_3\n",
      "0.47037037037037044\n",
      "flan_xl_few_specific_1\n",
      "0.4233009708737864\n",
      "flan_xl_few_specific_2\n",
      "0.4233009708737864\n",
      "flan_xl_few_specific_3\n",
      "0.4233009708737864\n",
      "flan_xl_counterfactual_default_1\n",
      "0.5261142431656206\n",
      "flan_xl_counterfactual_default_2\n",
      "0.5261142431656206\n",
      "flan_xl_counterfactual_default_3\n",
      "0.5261142431656206\n",
      "flan_xl_counterfactual_specific_1\n",
      "0.5004205214465938\n",
      "flan_xl_counterfactual_specific_2\n",
      "0.5004205214465938\n",
      "flan_xl_counterfactual_specific_3\n",
      "0.5004205214465938\n",
      "chatGPT_few_default_1\n",
      "0.3055913978494624\n",
      "chatGPT_few_default_2\n",
      "0.2608253968253968\n",
      "chatGPT_zero_default_1\n",
      "0.3370109546165884\n",
      "chatGPT_zero_default_2\n",
      "0.28794813119755913\n",
      "chatGPT_zero_specific_1\n",
      "0.2917403087074349\n",
      "chatGPT_zero_specific_2\n",
      "0.3121194029850747\n",
      "chatGPT_few_specific_1\n",
      "0.27156862745098037\n",
      "chatGPT_few_specific_2\n",
      "0.2755659640905543\n",
      "chatGPT_counterfactual_default_1\n",
      "0.19598714111517568\n",
      "chatGPT_counterfactual_default_2\n",
      "0.20452508960573476\n",
      "chatGPT_counterfactual_specific_1\n",
      "0.23589743589743586\n",
      "chatGPT_counterfactual_specific_2\n",
      "0.21219858156028368\n",
      "flan_xl_few_default_1\n",
      "0.4318507890961263\n",
      "flan_xl_few_default_2\n",
      "0.4318507890961263\n",
      "flan_xl_few_default_3\n",
      "0.4318507890961263\n",
      "flan_xl_zero_default_1\n",
      "0.4412191582002903\n",
      "flan_xl_zero_default_2\n",
      "0.4412191582002903\n",
      "flan_xl_zero_default_3\n",
      "0.4412191582002903\n",
      "flan_xl_zero_specific_1\n",
      "0.4412191582002903\n",
      "flan_xl_zero_specific_2\n",
      "0.4412191582002903\n",
      "flan_xl_zero_specific_3\n",
      "0.4412191582002903\n",
      "flan_xl_few_specific_1\n",
      "0.44638157894736846\n",
      "flan_xl_few_specific_2\n",
      "0.44638157894736846\n",
      "flan_xl_few_specific_3\n",
      "0.44638157894736846\n",
      "flan_xl_counterfactual_default_1\n",
      "0.4258241758241758\n",
      "flan_xl_counterfactual_default_2\n",
      "0.4258241758241758\n",
      "flan_xl_counterfactual_default_3\n",
      "0.4258241758241758\n",
      "flan_xl_counterfactual_specific_1\n",
      "0.4309460929004194\n",
      "flan_xl_counterfactual_specific_2\n",
      "0.4309460929004194\n",
      "flan_xl_counterfactual_specific_3\n",
      "0.4309460929004194\n",
      "chatGPT_few_default_1\n",
      "0.4352806414662085\n",
      "chatGPT_few_default_2\n",
      "0.44017607042817125\n",
      "chatGPT_zero_default_1\n",
      "0.4368415336157272\n",
      "chatGPT_zero_default_2\n",
      "0.45555555555555555\n",
      "chatGPT_zero_specific_1\n",
      "0.45597079037800686\n",
      "chatGPT_zero_specific_2\n",
      "0.45604238258877433\n",
      "chatGPT_few_specific_1\n",
      "0.4387997623291741\n",
      "chatGPT_few_specific_2\n",
      "0.46228239845261127\n",
      "chatGPT_counterfactual_default_1\n",
      "0.4604468599033816\n",
      "chatGPT_counterfactual_default_2\n",
      "0.4443923773820681\n",
      "chatGPT_counterfactual_specific_1\n",
      "0.47340067340067343\n",
      "chatGPT_counterfactual_specific_2\n",
      "0.4751322751322751\n",
      "flan_xl_few_default_1\n",
      "0.6998600746268655\n",
      "flan_xl_few_default_2\n",
      "0.6998600746268655\n",
      "flan_xl_few_default_3\n",
      "0.6998600746268655\n",
      "flan_xl_zero_default_1\n",
      "0.6996966632962589\n",
      "flan_xl_zero_default_2\n",
      "0.6996966632962589\n",
      "flan_xl_zero_default_3\n",
      "0.6996966632962589\n",
      "flan_xl_zero_specific_1\n",
      "0.6996966632962589\n",
      "flan_xl_zero_specific_2\n",
      "0.6996966632962589\n",
      "flan_xl_zero_specific_3\n",
      "0.6996966632962589\n",
      "flan_xl_few_specific_1\n",
      "0.7255184651214373\n",
      "flan_xl_few_specific_2\n",
      "0.7255184651214373\n",
      "flan_xl_few_specific_3\n",
      "0.7255184651214373\n",
      "flan_xl_counterfactual_default_1\n",
      "0.6774519716885743\n",
      "flan_xl_counterfactual_default_2\n",
      "0.6774519716885743\n",
      "flan_xl_counterfactual_default_3\n",
      "0.6774519716885743\n",
      "flan_xl_counterfactual_specific_1\n",
      "0.6774519716885743\n",
      "flan_xl_counterfactual_specific_2\n",
      "0.6774519716885743\n",
      "flan_xl_counterfactual_specific_3\n",
      "0.6774519716885743\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chatGPT_few_default_1</th>\n",
       "      <th>chatGPT_few_default_2</th>\n",
       "      <th>chatGPT_zero_default_1</th>\n",
       "      <th>chatGPT_zero_default_2</th>\n",
       "      <th>chatGPT_zero_specific_1</th>\n",
       "      <th>chatGPT_zero_specific_2</th>\n",
       "      <th>chatGPT_few_specific_1</th>\n",
       "      <th>chatGPT_few_specific_2</th>\n",
       "      <th>chatGPT_counterfactual_default_1</th>\n",
       "      <th>chatGPT_counterfactual_default_2</th>\n",
       "      <th>...</th>\n",
       "      <th>flan_xl_zero_specific_3</th>\n",
       "      <th>flan_xl_few_specific_1</th>\n",
       "      <th>flan_xl_few_specific_2</th>\n",
       "      <th>flan_xl_few_specific_3</th>\n",
       "      <th>flan_xl_counterfactual_default_1</th>\n",
       "      <th>flan_xl_counterfactual_default_2</th>\n",
       "      <th>flan_xl_counterfactual_default_3</th>\n",
       "      <th>flan_xl_counterfactual_specific_1</th>\n",
       "      <th>flan_xl_counterfactual_specific_2</th>\n",
       "      <th>flan_xl_counterfactual_specific_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>reddit</th>\n",
       "      <td>0.381</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasoc</th>\n",
       "      <td>0.306</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hateval</th>\n",
       "      <td>0.435</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         chatGPT_few_default_1  chatGPT_few_default_2  chatGPT_zero_default_1  \\\n",
       "reddit                   0.381                  0.324                   0.368   \n",
       "hasoc                    0.306                  0.261                   0.337   \n",
       "hateval                  0.435                  0.440                   0.437   \n",
       "\n",
       "         chatGPT_zero_default_2  chatGPT_zero_specific_1  \\\n",
       "reddit                    0.366                    0.561   \n",
       "hasoc                     0.288                    0.292   \n",
       "hateval                   0.456                    0.456   \n",
       "\n",
       "         chatGPT_zero_specific_2  chatGPT_few_specific_1  \\\n",
       "reddit                     0.367                   0.569   \n",
       "hasoc                      0.312                   0.272   \n",
       "hateval                    0.456                   0.439   \n",
       "\n",
       "         chatGPT_few_specific_2  chatGPT_counterfactual_default_1  \\\n",
       "reddit                    0.403                             0.501   \n",
       "hasoc                     0.276                             0.196   \n",
       "hateval                   0.462                             0.460   \n",
       "\n",
       "         chatGPT_counterfactual_default_2  ...  flan_xl_zero_specific_3  \\\n",
       "reddit                              0.352  ...                    0.470   \n",
       "hasoc                               0.205  ...                    0.441   \n",
       "hateval                             0.444  ...                    0.700   \n",
       "\n",
       "         flan_xl_few_specific_1  flan_xl_few_specific_2  \\\n",
       "reddit                    0.423                   0.423   \n",
       "hasoc                     0.446                   0.446   \n",
       "hateval                   0.726                   0.726   \n",
       "\n",
       "         flan_xl_few_specific_3  flan_xl_counterfactual_default_1  \\\n",
       "reddit                    0.423                             0.526   \n",
       "hasoc                     0.446                             0.426   \n",
       "hateval                   0.726                             0.677   \n",
       "\n",
       "         flan_xl_counterfactual_default_2  flan_xl_counterfactual_default_3  \\\n",
       "reddit                              0.526                             0.526   \n",
       "hasoc                               0.426                             0.426   \n",
       "hateval                             0.677                             0.677   \n",
       "\n",
       "         flan_xl_counterfactual_specific_1  flan_xl_counterfactual_specific_2  \\\n",
       "reddit                               0.500                              0.500   \n",
       "hasoc                                0.431                              0.431   \n",
       "hateval                              0.677                              0.677   \n",
       "\n",
       "         flan_xl_counterfactual_specific_3  \n",
       "reddit                               0.500  \n",
       "hasoc                                0.431  \n",
       "hateval                              0.677  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "columns = ['chatGPT_few_default_1', 'chatGPT_few_default_2',\n",
    "       'chatGPT_zero_default_1', 'chatGPT_zero_default_2',\n",
    "       'chatGPT_zero_specific_1', 'chatGPT_zero_specific_2',\n",
    "       'chatGPT_few_specific_1', 'chatGPT_few_specific_2',\n",
    "       'chatGPT_counterfactual_default_1', 'chatGPT_counterfactual_default_2',\n",
    "       'chatGPT_counterfactual_specific_1',\n",
    "       'chatGPT_counterfactual_specific_2', 'flan_xl_few_default_1', 'flan_xl_few_default_2', 'flan_xl_few_default_3', 'flan_xl_zero_default_1', 'flan_xl_zero_default_2', 'flan_xl_zero_default_3', 'flan_xl_zero_specific_1', 'flan_xl_zero_specific_2', 'flan_xl_zero_specific_3', 'flan_xl_few_specific_1', 'flan_xl_few_specific_2', 'flan_xl_few_specific_3', 'flan_xl_counterfactual_default_1', 'flan_xl_counterfactual_default_2','flan_xl_counterfactual_default_3', 'flan_xl_counterfactual_specific_1','flan_xl_counterfactual_specific_2','flan_xl_counterfactual_specific_3']\n",
    "dfs = [reddit_full_labeled, hasoc_full_labeled, hateval_full_labeled]\n",
    "for df in dfs:\n",
    "    results = []\n",
    "    for model in columns:\n",
    "        print(model)\n",
    "        f1 = f1_score(df['class'], df[model], average='macro')\n",
    "        print(f1)\n",
    "        results.append(round(f1, 3)) \n",
    "    f1_scores.append(results)\n",
    "\n",
    "\n",
    "\n",
    "# flan_columns = ['flan_xl_few_default_1', 'flan_xl_few_default_2', 'flan_xl_few_default_3', 'flan_xl_zero_default_1', 'flan_xl_zero_default_2', 'flan_xl_zero_default_3', 'flan_xl_zero_specific_1', 'flan_xl_zero_specific_2', 'flan_xl_zero_specific_3', 'flan_xl_few_specific_1', 'flan_xl_few_specific_2', 'flan_xl_few_specific_3', 'flan_xl_counterfactual_default_1', 'flan_xl_counterfactual_default_2','flan_xl_counterfactual_default_3', 'flan_xl_counterfactual_specific_1','flan_xl_counterfactual_specific_2','flan_xl_counterfactual_specific_3']\n",
    "# gpt_columns = ['chatGPT_few_default_1', 'chatGPT_few_default_2',\n",
    "    #    'chatGPT_zero_default_1', 'chatGPT_zero_default_2',\n",
    "    #    'chatGPT_zero_specific_1', 'chatGPT_zero_specific_2',\n",
    "    #    'chatGPT_few_specific_1', 'chatGPT_few_specific_2',\n",
    "    #    'chatGPT_counterfactual_default_1', 'chatGPT_counterfactual_default_2',\n",
    "    #    'chatGPT_counterfactual_specific_1',\n",
    "    #    'chatGPT_counterfactual_specific_2']\n",
    "\n",
    "# results_gab = []\n",
    "# for model in gpt_columns:\n",
    "#     f1 = f1_score(gab_full_labeled['class'], df[model], average='macro')\n",
    "#     results_gab.append(round(f1, 3))\n",
    "# for model in flan_columns:\n",
    "#     f1 = f1_score(gab_flan['class'], df[model], average='macro')\n",
    "#     results_gab.append(round(f1, 3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f1_scores_df = pd.DataFrame(f1_scores, columns = columns, index = ['reddit', 'hasoc', 'hateval'])\n",
    "f1_scores_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do your results for mode A have the same performance as those reported in the paper (figure 1)? [2 points]**\n",
    "\n",
    "\n",
    "**Is one of the LLMs better than the other one? [2 points]**\n",
    "Flan is much more consistent across all of the runs, which is a very good property to have. I would rather have a more consistent if not as accurate model versus a potetially better model that has a larger varience in its answers. The values are also higher or at least very similar between Flan and chatGPT.\n",
    "\n",
    "\n",
    "**Does varying the definition improve performance? [2 points]**\n",
    "Yes, the few shot models responded very positively to a more precise definition of the dataset. This was a definition I came up with through reading the papers and also adding some of my own research into the platforms where the data came from. The counterfactual models don't really have much of a difference between the default and the specific definitions as the counterfactuals \n",
    "\n",
    "**Which prompting technique — zero-shot, few-shot — works best? [2 points] Bonus: also compare against counterfactual prompting [Bonus 1 point]**\n",
    "Few shot consistenly outperforms zero shot. This makes sense as we are basically expanding upon the definition of the task we want the LLMs to perform. Counterfactual did better than both of these however, as we show what specific words or phrases are considered hate speech. For example, I mostly added/subtracted slurs and rude language, which is a prominent feature of hate speech. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
