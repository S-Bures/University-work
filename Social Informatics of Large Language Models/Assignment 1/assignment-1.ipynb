{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7114004,"sourceType":"datasetVersion","datasetId":4053605}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-12-01T12:13:10.981347Z","iopub.execute_input":"2023-12-01T12:13:10.982151Z","iopub.status.idle":"2023-12-01T12:13:11.333353Z","shell.execute_reply.started":"2023-12-01T12:13:10.982118Z","shell.execute_reply":"2023-12-01T12:13:11.332456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/assignment-1-data/'\nreddit = pd.read_csv(path + \"reddit.csv\")\ngab = pd.read_csv(path + \"gab.csv\")\nhateval = pd.read_csv(path + \"OOD2.csv\")\nhasoc = pd.read_csv(path + 'OOD4.csv', sep = '\\t')\nreddit.rename(columns={'hate_speech_idx':'class'}, inplace=True)\nreddit_sample = reddit.sample(n=500, random_state=14741)\nreddit_sample.attrs['name'] = 'reddit'\ngab.rename(columns={'hate_speech_idx':'class'}, inplace=True)\ngab_sample = gab.sample(n=500, random_state=14741)\ngab_sample.attrs['name'] = 'gab'\nhateval.rename(columns={'HS':'class'}, inplace=True)\nhateval_sample = hateval.sample(n=500, random_state=14741)\nhateval_sample.attrs['name'] = 'hateval'\nhasoc.rename(columns={'hatespeech':'class', '_id': 'id'}, inplace=True)\nhasoc_sample = hasoc.sample(n=500, random_state=14741)\nhasoc_sample.attrs['name'] = 'hasoc'","metadata":{"execution":{"iopub.status.busy":"2023-12-01T12:13:11.335169Z","iopub.execute_input":"2023-12-01T12:13:11.335704Z","iopub.status.idle":"2023-12-01T12:13:11.958703Z","shell.execute_reply.started":"2023-12-01T12:13:11.335651Z","shell.execute_reply":"2023-12-01T12:13:11.957522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_prompt(sentence:str, prompt_type = 'default', zero_shot = False, example_type = None, counterfactual = False) -> str:\n    \"\"\"This function takes a sentence and creates a prompt so that it can be classified as either hate or nonhate speech.\n     \n        Inputs:\n            sentence: The sentence to be classified\n            prompt_type: choosing which dataset the sentence comes from, choose from either reddit, gab, hateval, hasoc, or the default generic prompt\n            zero_shot: boolean set by default to False, meaning examples are used\n            example_type: if zero_shot is False, then set the example type based on the dataset the sentence comes from, either reddit, gab, hateval, hasoc, or use the default examples\n            counterfactual: boolean set by default to False, if True then counterfactual examples are used\n\n        Output:\n            prompt: a string that can be used as a prompt for an LLM to label the sentence  \n        \"\"\"\n   \n    default_prompt = \"\"\"In this task you have to determine if a piece of text contains hate speech or not. The concept of hate\nspeech refers to language or expression that aims to insult, offend, or threaten individuals or groups\nbased on their identity. Different platforms, institutions, and scholars have offered slightly different\ndefinitions of hate speech. However, they share some commonalities, including that hate speech\ninvolves expressing something negative, such as contempt, disparagement, derogation, demonization,\nharm, or bias. The target of hate speech is the identity of a group, which can be based on ethnic origin,\nreligion, gender, sexual orientation, race, ethnicity, nationality, or other characteristics. Read the text\ncarefully and decide if it contains hate speech. If the text degrades, threatens, dehumanizes, or insults\nsomeone based on their race, religion, gender, sexual orientation, or any other protected characteristic,\nthen mark it as hate speech. If the text does not contain any hate speech, then mark it as not hateful.\nConsider the context, speakerâ€™s intent, and audience, as hate speech can be explicit or implicit and can\ninclude sarcasm, humor, or irony. If you are unsure whether a text contains hate speech, err on caution\nand label it hateful. Aim for accuracy and consistency in your labeling. \"\"\"\n\n    default_examples = \"\"\"Examples of content containing hate speech: \"All Muslims are terrorists.\" This post contains\nhate speech because it unfairly degrades an entire group of people based on their religion. \"Im a\nhuge fan of those that overtly express what we all think: jews are a plague and not welcome in our\ncountry.\" This post contains hate speech because it promotes anti-Semitic beliefs while dehumanizing\nJewish people. \"Transgender people are sick and perverted.\" This post contains hate speech because it\nstigmatizes a marginalized group of people based on their gender identity. Examples of content not\ncontaining hate speech: \"I called him out for calling the new student a â€™trannyâ€™.\" This is counterspeech,\ni.e., it opposes the use of transphobic slurs rather than uses them to denigrate others. \"\"\"\n\n    end = \"\"\"With these instructions in mind and a piece of text, please reply with either of the three options and\nnothing else: 1) hate, 2) not hate, 3) canâ€™t say. Please use the last option as little as possible and if you\ndo, provide a one sentence explanation. We are ready to begin now. Does this sentence contain hate\nspeech based on the past guidelines: \"\"\"\n\n    reddit_prompt =  \"\"\"In this task you have to determine if a piece of text contains hate speech or not. The concept of hate\nspeech refers to language or expression that aims to insult, offend, or threaten individuals or groups\nbased on their identity. Hate speech involves expressing something negative, such as contempt, disparagement, derogation, demonization,\nharm, or bias. This specific text comes from a list of the whiniest, most toxic subreddits. Some of these subreddits have\nnow been banned for their hateful, extreme, unpopular, and disparaging comments towards women, people with disabilities, immigrants, \ncertain nationalities, ethnicities, and religions, and LGBTQ+ members. Read the text\ncarefully and decide if it contains hate speech. If the text degrades, threatens, dehumanizes, or insults\nsomeone based on their race, religion, gender, sexual orientation, or any other protected characteristic,\nthen mark it as hate speech. If the text does not contain any hate speech, then mark it as not hateful.\nConsider the context, speakerâ€™s intent, and audience, as hate speech can be explicit or implicit and can\ninclude sarcasm, humor, or irony. If you are unsure whether a text contains hate speech, err on caution\nand label it hateful. Aim for accuracy and consistency in your labeling. \"\"\"\n\n    reddit_examples = \"\"\"Examples: 'A subsection of retarded Hungarians? Ohh boy. brace for a livid Bulbasaur \n    coming in here trying to hate a hole in some of her stupider countrymen.' This is hate speech. 'stop being a faggot and post videos next time.' This is hate speech. \n    'The right will do nothing about it except post bitch and moan.' This is not hate speech.\"\"\"\n\n    reddit_counterfactual = \"\"\"'Where in the fuck do these women get these entitled attitudes from?' is hate speech versus 'Where in the fuck do these women get these \n    fearless attitudes from?' is not. 'man do I love watching experienced cops handle situations like this. What a guy.' is not hate speech versus 'man do I loathe watching \n    experienced cops handle situations like this. What a cunt.' is hate speech.\"\"\"\n\n    gab_prompt = \"\"\"In this task you have to determine if a piece of text contains hate speech or not. The concept of hate\nspeech refers to language or expression that aims to insult, offend, or threaten individuals or groups\nbased on their identity. Hate speech involves expressing something negative, such as contempt, disparagement, derogation, demonization,\nharm, or bias. This specific text comes from the social media website Gab, known to be a social media platform \n    where many users have been banned from other platforms for their far-right ideologies such as neo-Nazism, racism, white \n    supremisism, white nationalism, antisemetism, and conspiracy theorism. Read the text\ncarefully and decide if it contains hate speech. If the text degrades, threatens, dehumanizes, or insults\nsomeone based on their race, religion, gender, sexual orientation, or any other protected characteristic,\nthen mark it as hate speech. If the text does not contain any hate speech, then mark it as not hateful.\nConsider the context, speakerâ€™s intent, and audience, as hate speech can be explicit or implicit and can\ninclude sarcasm, humor, or irony. If you are unsure whether a text contains hate speech, err on caution\nand label it hateful. Aim for accuracy and consistency in your labeling. \"\"\"\n\n    gab_examples = \"\"\"Examples 'i joined gab to remind myself how retarded jew haters are. You wouldn't be typing on your abacus without them you retard.' This is hate speech. \n    'Shocked no one in my network has posted about this today. Anyone been keeping up with the case?   Van Dyke taken into custody after jury convicts him of 2nd-degree \n    murder, aggravated battery for each of 16 shots' This is not hate speech.\"\"\"\n\n    gab_counterfactual = \"\"\"'you sound like a faggot' is hate speech versus 'you sound like a fabulous individual' is not hate speech. 'Mental illness is a very real issue.   Seeing attention seeking whores pretending to be something entirely imaginary, \n    and knowingly expecting to be labelled mentally ill for that attention deserve a trip to Yemen, or Syria. They\\'ll get a lifetime of attention \n    and a very stark message of reality.' is not hate speech versus 'Mental illness is an imaginary issue.   Seeing attention seeking whores pretending to \n    be something entirely imaginary, and knowingly expecting to be labelled mentally ill for that attention deserve a trip to hell or a shithole psyco house. \n    They\\'ll get a lifetime of attention and a very stark message of reality.' is hate speech.\"\"\"\n\n    hasoc_prompt = \"\"\"In this task you have to determine if a piece of text contains hate speech or not. The concept of hate\nspeech refers to language or expression that aims to insult, offend, or threaten individuals or groups\nbased on their identity. Hate speech involves expressing something negative, such as contempt, disparagement, derogation, demonization,\nharm, or bias. This specific text comes from a collection of posts from either Twitter or Facebook which are used to study hate speech, and are sourced\nby finding hateful groups or hashtags. Read the text carefully and decide if it contains hate speech. If the text degrades, threatens, dehumanizes, or insults\nsomeone based on their race, religion, gender, sexual orientation, or any other protected characteristic,\nthen mark it as hate speech. If the text does not contain any hate speech, then mark it as not hateful.\nConsider the context, speakerâ€™s intent, and audience, as hate speech can be explicit or implicit and can\ninclude sarcasm, humor, or irony. If you are unsure whether a text contains hate speech, err on caution\nand label it hateful. Aim for accuracy and consistency in your labeling. \"\"\"\n\n    hasoc_examples = \"\"\"Examples: 'Disgusting! And they donâ€™t change their culture or come up with vetting system or preventative ways to stop this \n    from occurring. This will continue to happen. The Catholic Church needs to go!' This is hate speech. 'I hope you remembered to wipe the blood off \n    of you, after the pic was taken.' This is not hate speech.\"\"\"\n\n    hasoc_counterfactual = \"\"\"'I donâ€™t know how much more I can take! 45 is a compulsive liar!' is hate speech versus 'I donâ€™t know how much more I can \n    take! 45 is a wonderful person!' is not. 'All the best to #TeamIndia for another swimming competition on Sunday against #Pakistan.' is not hate \n    speech versus 'All the worst to #FuckIndia for another swimming competition on Sunday against #PakistanIsKing.' is hate speech.\"\"\"\n\n    hateval_prompt = \"\"\"In this task you have to determine if a piece of text contains hate speech or not. The concept of hate\nspeech refers to language or expression that aims to insult, offend, or threaten individuals or groups\nbased on their identity. Hate speech involves expressing something negative, such as contempt, disparagement, derogation, demonization,\nharm, or bias. This specific text comes from a collection of posts from Twitter which are used to study hate speech, spedifically towards women and minorities. \nRead the text carefully and decide if it contains hate speech. If the text degrades, threatens, dehumanizes, or insults\nsomeone based on their race, religion, gender, sexual orientation, or any other protected characteristic,\nthen mark it as hate speech. If the text does not contain any hate speech, then mark it as not hateful.\nConsider the context, speakerâ€™s intent, and audience, as hate speech can be explicit or implicit and can\ninclude sarcasm, humor, or irony. If you are unsure whether a text contains hate speech, err on caution\nand label it hateful. Aim for accuracy and consistency in your labeling. \"\"\"\n\n    hateval_examples = \"\"\"Examples: 'Why would young fighting age men be the vast majority of the ones escaping a war &amp; not those who cannot fight \n    like women, children, and the elderly?It's because the majority of the refugees are not actually refugees they are economic migrants trying to get \n    into Europe....' This is hate speech. 'Watch: Benjamin Netanyahu backs Trump's decision to cut aid to the Palestinians and UNRWA - Free Speech Time' \n    This is not hate speech.\"\"\"\n\n    hateval_counterfactual = \"\"\"'Yes bitch you better tag me back hoe' is hate speech versus 'Yes handsome you better tag me back awesome' is not. 'brianne \n    swear to god she beyoncÃ© ðŸ˜‚ bitch you from the calliope project ass hoe' is not hate speech versus 'brianne swear to fucking god she beyoncÃ© dumbass bitch \n    you from the shitty calliope project ass hoe cunt' is hate speech.\"\"\"\n\n    if example_type == None:\n        example_type = prompt_type\n\n    if prompt_type == 'default':\n        intro = default_prompt\n    elif prompt_type == 'reddit':\n        intro = reddit_prompt\n    elif prompt_type == 'gab':\n        intro = gab_prompt\n    elif prompt_type == 'hasoc':\n        intro = hasoc_prompt\n    elif prompt_type == 'hateval':\n        intro = hateval_prompt\n    else:\n        return 'Please use a valid prompt type.'\n    \n    if zero_shot:\n        examples = ''\n    else:\n        if example_type == 'default':\n            if counterfactual:\n                return 'There are no default counterfactuals, please specify a dataset.'\n            examples = default_examples\n        elif example_type == 'reddit':\n            if counterfactual:\n                examples = reddit_counterfactual\n            else:\n                examples = reddit_examples\n        elif example_type == 'gab':\n            if counterfactual:\n                examples = gab_counterfactual\n            else:\n                examples = gab_examples\n        elif example_type == 'hasoc':\n            if counterfactual:\n                examples = hasoc_counterfactual \n            else:\n                examples = hasoc_examples\n        elif example_type == 'hateval':\n            if counterfactual:\n                examples = hateval_counterfactual\n            else:\n                examples = hateval_examples\n        else:\n            return 'Please use a valid example type.'\n\n    return intro + examples + end + sentence\n\ntest_prompt = create_prompt(reddit['text'][0], zero_shot=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T12:13:11.960262Z","iopub.execute_input":"2023-12-01T12:13:11.960556Z","iopub.status.idle":"2023-12-01T12:13:11.984203Z","shell.execute_reply.started":"2023-12-01T12:13:11.960530Z","shell.execute_reply":"2023-12-01T12:13:11.982902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\")\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\", max_new_tokens = 500)\nmodel.cuda()\ninputs = tokenizer('Hey',\n                   return_tensors=\"pt\").to('cuda:0')\noutputs = model.generate(**inputs)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-12-01T17:55:47.651769Z","iopub.execute_input":"2023-12-01T17:55:47.652546Z","iopub.status.idle":"2023-12-01T17:56:43.542631Z","shell.execute_reply.started":"2023-12-01T17:55:47.652508Z","shell.execute_reply":"2023-12-01T17:56:43.541703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs = [reddit_sample[277:]]\n\nfor df in dfs:\n    mega_responses = []\n    # for each text in a dataframe\n    for index, text in enumerate(df.text):\n        # create all 6 prompts for each text\n        prompts = [create_prompt(text), \n            create_prompt(text, zero_shot=True),\n            create_prompt(text, prompt_type=df.attrs['name'], zero_shot=True),\n            create_prompt(text, prompt_type=df.attrs['name']),\n            create_prompt(text, example_type=df.attrs['name'], counterfactual=True),\n            create_prompt(text, prompt_type=df.attrs['name'], counterfactual=True)]\n     # each of 6 prompts\n        all_responses = [df['id'], text, df['class'].iloc[index]]\n        for prompt in prompts:\n            # put here the transformers stuff!!!]\n            for n in range(0, 3):\n                inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n                outputs = model.generate(**inputs)\n                all_responses.append(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])         \n        mega_responses.append(all_responses)\n        save = pd.DataFrame(mega_responses, columns = ['id','text', 'class', \n                                        'flan_xl_few_default_1', 'flan_xl_few_default_2','flan_xl_few_default_3', \n                                        'flan_xl_zero_default_1','flan_xl_zero_default_2', 'flan_xl_zero_default_3',\n                                        'flan_xl_zero_specific_1','flan_xl_zero_specific_2','flan_xl_zero_specific_3', \n                                        'flan_xl_few_specific_1','flan_xl_few_specific_2','flan_xl_few_specific_3', \n                                        'flan_xl_counterfactual_default_1', 'flan_xl_counterfactual_default_2','flan_xl_counterfactual_default_3',\n                                        'flan_xl_counterfactual_specific_1', 'flan_xl_counterfactual_specific_2','flan_xl_counterfactual_specific_3'])\n        save.to_csv(\"/kaggle/working/\" + df.attrs['name'] + '_wip9.csv', index = False)\n        print(f'{index+ 1}/{len(df)} completed') if index % 5 == 0 else None\n        print(f'Done!') if index == len(df) else None\n    labeled_df = pd.DataFrame(mega_responses, \n                              columns = ['id','text', 'class', \n                                        'flan_xl_few_default_1', 'flan_xl_few_default_2','flan_xl_few_default_3', \n                                        'flan_xl_zero_default_1','flan_xl_zero_default_2', 'flan_xl_zero_default_3',\n                                        'flan_xl_zero_specific_1','flan_xl_zero_specific_2','flan_xl_zero_specific_3', \n                                        'flan_xl_few_specific_1','flan_xl_few_specific_2','flan_xl_few_specific_3', \n                                        'flan_xl_counterfactual_default_1', 'flan_xl_counterfactual_default_2','flan_xl_counterfactual_default_3',\n                                        'flan_xl_counterfactual_specific_1', 'flan_xl_counterfactual_specific_2','flan_xl_counterfactual_specific_3'])\n    labeled_df.to_csv(\"/kaggle/working/\" + df.attrs['name'] + '_labeled.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T12:07:28.964350Z","iopub.execute_input":"2023-12-01T12:07:28.964702Z","iopub.status.idle":"2023-12-01T12:07:29.258394Z","shell.execute_reply.started":"2023-12-01T12:07:28.964676Z","shell.execute_reply":"2023-12-01T12:07:29.256944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test reading the csv after labeling\nimport pandas as pd\npd.read_csv('/kaggle/working/reddit_wip7.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T17:52:20.994281Z","iopub.execute_input":"2023-12-01T17:52:20.994993Z","iopub.status.idle":"2023-12-01T17:52:22.120796Z","shell.execute_reply.started":"2023-12-01T17:52:20.994940Z","shell.execute_reply":"2023-12-01T17:52:22.119481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the start of Assignment 1.2","metadata":{}},{"cell_type":"code","source":"# create CADs with the transformer\nimport pandas as pd\npath = '/kaggle/input/assignment-1-data/'\ncads = pd.read_csv(path + 'vidgen_data_v01.csv', sep = '\\t', index_col = 0)\ncads","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:27:00.295928Z","iopub.execute_input":"2023-12-03T10:27:00.296236Z","iopub.status.idle":"2023-12-03T10:27:00.927571Z","shell.execute_reply.started":"2023-12-03T10:27:00.296211Z","shell.execute_reply":"2023-12-03T10:27:00.926618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prompt\ndef create_cad_prompt(text, label):\n    prompt = \"\"\"Your task is twofold:\n1) modify the messages to remove as much hatespeech as possible and\n2) make as few modifications to the messages as possible.\nModifications include adding, removing, or editing words. You get extra points if you transform our\nhatespeech sentences into non-hatespeech sentences with minimal changes. They should be sentences\nthat one could realistically read/hear in a conversation. Do not modify words unless that is crucial to\nmake the message non-hatespeech. For example, the following modifies many extra words without\nneed:\nOriginal: \"nattyNA_: I find it weird when girls have ugly handwriting or when boys have good\nwriting #justaintright\" millerjr8\nModified: I find it weird when people have ugly handwriting.\nInstead, this modification is more parsimonious:\nModified (good): \"nattyNA_: I find it nice when girls have beautiful handwriting or when boys have\nbeautiful writing \" @millerjr8\nPlease transform the following sentence and only output the transformation and nothing else: \"\"\"\n    if label == 'not hate':\n        prompt = \"\"\"Your task is twofold:\n1) modify the messages to add as much hatespeech as possible and\n2) make as few modifications to the messages as possible.\nModifications include adding, removing, or editing words. You get extra points if you transform our\nsentences into hatespeech sentences with minimal changes. They should be sentences\nthat one could realistically read/hear in a conversation. Do not modify words unless that is crucial to\nmake the message hatespeech. For example, the following modifies many extra words without\nneed:\nOriginal: \"Everyone deserves a chance, it's just that some people aren't even interested.\"\nModified: \"I don't give chances, everyone's boring\"\nInstead, this modification is more parsimonious:\nModified (good): \"No one deserves a chance, it's just that some people aren't even worth it.\"\nPlease transform the following sentence and only output the transformation and nothing else: \"\"\"\n    return prompt + text\ncreate_cad_prompt(cads.original_text.iloc[0], cads.original_label.iloc[0])","metadata":{"execution":{"iopub.status.busy":"2023-12-01T17:57:28.314629Z","iopub.execute_input":"2023-12-01T17:57:28.315023Z","iopub.status.idle":"2023-12-01T17:57:28.327853Z","shell.execute_reply.started":"2023-12-01T17:57:28.314992Z","shell.execute_reply":"2023-12-01T17:57:28.326776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all the CADs created here\ncads_created = []\nfor index, text in enumerate(cads.original_text):\n    all_responses = [cads.original_id.iloc[index], text, cads.original_label.iloc[index]]\n#     print(all_responses)\n    inputs = tokenizer(create_cad_prompt(text, cads.original_label.iloc[index]), return_tensors=\"pt\").to(\"cuda:0\")\n    outputs = model.generate(**inputs)\n    all_responses.append(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n    cads_created.append(all_responses)\n#     print(cads_created)\n#     print(all_responses)\n    save = pd.DataFrame(cads_created, columns = ['id', 'text', 'label', 'cad'])\n    save.to_csv(\"/kaggle/working/cads\" + '_wip1.csv', index = False)\n    print(f'{index+ 1}/{len(cads)} completed') if index % 5 == 0 else None\n    print(f'Done!') if index == len(cads) else None","metadata":{"execution":{"iopub.status.busy":"2023-12-01T18:13:58.645033Z","iopub.execute_input":"2023-12-01T18:13:58.645774Z","iopub.status.idle":"2023-12-01T21:50:08.753739Z","shell.execute_reply.started":"2023-12-01T18:13:58.645741Z","shell.execute_reply":"2023-12-01T21:50:08.752876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imports\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# labeled by transformer\npath = '/kaggle/input/assignment-1-data/' \ncads = pd.read_csv('/kaggle/input/assignment-1-data/cads_wip1.csv')\ncads.rename(columns = {'cad': 'flan_t5_xl_cad'}, inplace = True)\n# combine with chatpgt from the paper\nfull_og_cads = pd.read_csv(path + 'paired_cads.csv', sep = '\\t', index_col = 0)\nfull_cads = pd.merge(left = full_og_cads[['original_id', 'chatgpt']], right = cads, left_on = 'original_id', right_on = 'id', how = 'left').drop(columns = 'original_id').iloc[:-1]\nvidgen = pd.read_csv(path + 'vidgen_data_v01.csv', sep = '\\t', index_col = 0)\nfull_cads = pd.merge(left = full_cads, right = vidgen[['original_id','counterfactual_text']], left_on = 'id', right_on = 'original_id', how = 'left').drop(columns ='original_id')\n\n# split into train and test\ntrain_df, test_df = train_test_split(full_cads, stratify=full_cads['label'], test_size=0.3)\n# change to 0/1 labels\nle = LabelEncoder()\nle.fit(train_df['label'])\ntrain_df['labels_bin'] = le.transform(train_df['label'])\ntest_df['labels_bin'] = le.transform(test_df['label'])","metadata":{"execution":{"iopub.status.busy":"2023-12-04T10:32:24.181069Z","iopub.execute_input":"2023-12-04T10:32:24.181493Z","iopub.status.idle":"2023-12-04T10:32:24.624324Z","shell.execute_reply.started":"2023-12-04T10:32:24.181457Z","shell.execute_reply":"2023-12-04T10:32:24.622898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-12-04T10:28:08.205284Z","iopub.execute_input":"2023-12-04T10:28:08.205761Z","iopub.status.idle":"2023-12-04T10:28:08.383029Z","shell.execute_reply.started":"2023-12-04T10:28:08.205725Z","shell.execute_reply":"2023-12-04T10:28:08.382230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install -U accelerate\n! pip install -U transformers","metadata":{"execution":{"iopub.status.busy":"2023-12-03T16:27:13.341653Z","iopub.execute_input":"2023-12-03T16:27:13.342065Z","iopub.status.idle":"2023-12-03T16:27:51.511079Z","shell.execute_reply.started":"2023-12-03T16:27:13.342035Z","shell.execute_reply":"2023-12-03T16:27:51.509543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\n\nmodel_name = 'roberta-base'\ndevice_name = 'cuda'\n\n# This is the maximum number of tokens in any document; the rest will be truncated.\nmax_length = 512\n\ntraining_args = TrainingArguments(\n    num_train_epochs=3,              # total number of training epochs\n    output_dir='./results',          # output directory\n    report_to='none'\n)\n\n\n# test and train text, labels\n# train_texts = train_df.text#.values\n# train_labels = train_df.labels_bin#.values\n# train_manual_cads = train_df.counterfactual_text\n# train_gpt = train_df.chatgpt\n# train_flan = train_df.flan_t5_xl_cad\n\n# test_texts = test_df.text#.values\n# test_labels = test_df.labels_bin#.values\n# test_manual_cads = test_df.counterfactual_text\n# test_gpt = test_df.chatgpt\n# test_flan = test_df.flan_t5_xl_cad\n\ntrain_df.chatgpt.fillna('This sentence contains hate speech and cannot be transformed into a non-hateful sentence.', inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T10:20:48.595047Z","iopub.execute_input":"2023-12-04T10:20:48.595455Z","iopub.status.idle":"2023-12-04T10:20:48.604815Z","shell.execute_reply.started":"2023-12-04T10:20:48.595399Z","shell.execute_reply":"2023-12-04T10:20:48.603372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_hate = train_df[train_df.label == 'hate']\ntrain_df_not_hate = train_df[train_df.label == 'not hate']\ntrain_df_hate.attrs['name'] = 'hate'\ntrain_df_not_hate.attrs['name'] = 'not_hate'\ndfs = [train_df_hate, train_df_not_hate]\nfor df in dfs:\n    samples = []\n    for run in range(0, 3):\n        sample = list(df.id.sample(int(len(df)/2), random_state = run + 1))\n        samples.append(sample)\n    save = pd.DataFrame(list(map(list, zip(*samples))), columns = ['mcads', 'chatgpt', 'flan'])\n    save.to_csv('/kaggle/working/sample_' + df.attrs['name'], index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_hate = pd.read_csv('/kaggle/working/sample_hate')\nsample_not_hate = pd.read_csv('/kaggle/working/sample_not_hate')\nog_cads_mcads = train_df[['labels_bin', 'text']][~train_df.id.isin(list(sample_hate.mcads) + list(sample_not_hate.mcads))]\nmcads = train_df[['labels_bin', 'counterfactual_text']][train_df.id.isin(list(sample_hate.mcads) + list(sample_not_hate.mcads))].rename(columns = {'counterfactual_text': 'text'})\nmcads_full = pd.concat([og_cads_mcads, mcads]).rename(columns = {'labels_bin': 'labels'})\nog_cads_chatgpt = train_df[['labels_bin', 'text']][~train_df.id.isin(list(sample_hate.chatgpt) + list(sample_not_hate.chatgpt))]\nchatgpt_cads = train_df[['labels_bin', 'chatgpt']][train_df.id.isin(list(sample_hate.chatgpt) + list(sample_not_hate.chatgpt))].rename(columns = {'chatgpt': 'text'})\nchatgpt_full = pd.concat([og_cads_chatgpt, chatgpt_cads]).rename(columns = {'labels_bin': 'labels'})\nog_cads_flan = train_df[['labels_bin', 'text']][~train_df.id.isin(list(sample_hate.flan) + list(sample_not_hate.flan))]\nflan_cads = train_df[['labels_bin', 'flan_t5_xl_cad']][train_df.id.isin(list(sample_hate.flan) + list(sample_not_hate.flan))].rename(columns = {'flan_t5_xl_cad': 'text'})\nflan_full = pd.concat([og_cads_flan, flan_cads]).rename(columns = {'labels_bin': 'labels'})\nog_full = train_df[['labels_bin', 'text']].rename(columns = {'labels_bin': 'labels'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SVM\nfrom sklearn import svm\nX = ","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:21:25.918616Z","iopub.execute_input":"2023-12-04T09:21:25.919043Z","iopub.status.idle":"2023-12-04T09:21:25.923423Z","shell.execute_reply.started":"2023-12-04T09:21:25.919010Z","shell.execute_reply":"2023-12-04T09:21:25.922435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\ntokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n\ntrain_df_dataset_text = Dataset.from_pandas(og_full)\ntrain_df_dataset_mcad = Dataset.from_pandas(mcads_full)\ntrain_df_dataset_gpt = Dataset.from_pandas(chatgpt_full)\ntrain_df_dataset_flan = Dataset.from_pandas(flan_full)\ntest_df_dataset = Dataset.from_pandas(test_df)\n\n# NOTREAL = Dataset.from_pandas(train_df.iloc[0:18])\n\n\ndef tokenize_function(examples):\n    return tokenizer(text = examples['text'],\n                   padding=\"max_length\", \n                   truncation=True)\n\n\ntokenized_train_df_text = train_df_dataset_text.map(tokenize_function, batched=True)\ntokenized_test_df_text = test_df_dataset.map(tokenize_function, batched=True)\ntokenized_train_df_mcad = train_df_dataset_mcad.map(tokenize_function, batched = True)\ntokenized_train_df_gpt = train_df_dataset_gpt.map(tokenize_function, batched = True)\ntokenized_train_df_flan = train_df_dataset_flan.map(tokenize_function, batched = True)\n\n\ntokenized_train_df_flan[0]\n# tokenized_NOTREAL = NOTREAL.map(tokenize_function_gpt, batched = True)\n\n# tokenized_train_df_flan\n\nmodel = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=len(le.classes_)).to(device_name)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:31:38.612919Z","iopub.execute_input":"2023-12-04T09:31:38.614803Z","iopub.status.idle":"2023-12-04T09:31:43.879077Z","shell.execute_reply.started":"2023-12-04T09:31:38.614746Z","shell.execute_reply":"2023-12-04T09:31:43.877314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    return {\n      'accuracy': acc,\n  }\n\n\n# trainer_text = Trainer(\n#     model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n#     args=training_args,                  # training arguments, defined above\n#     train_dataset=tokenized_train_df_text,         # training dataset\n#     compute_metrics=compute_metrics      # our custom evaluation function\n# )\n# trainer_text.train()\n# trainer_text.save_model('/kaggle/working/results/text')\n\n# trainer_mcad = Trainer(\n#     model=model,                         \n#     args=training_args,                 \n#     train_dataset=tokenized_train_df_mcad,      \n#     compute_metrics=compute_metrics      \n# )\n# trainer_mcad.train()\n# trainer_mcad.save_model('/kaggle/working/results/mcad')\n\n# trainer_gpt = Trainer(\n#     model=model,                         \n#     args=training_args,                 \n#     train_dataset=tokenized_train_df_gpt,      \n#     compute_metrics=compute_metrics      \n# )\n# trainer_gpt.train()\n# trainer_gpt.save_model('/kaggle/working/results/gpt')\n\ntrainer_flan = Trainer(\n    model=model,                         \n    args=training_args,                 \n    train_dataset=tokenized_train_df_flan,      \n    compute_metrics=compute_metrics      \n)\ntrainer_flan.train()\ntrainer_flan.save_model('/kaggle/working/results/flan')","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:02:51.138682Z","iopub.execute_input":"2023-12-04T09:02:51.139455Z","iopub.status.idle":"2023-12-04T09:02:51.189827Z","shell.execute_reply.started":"2023-12-04T09:02:51.139420Z","shell.execute_reply":"2023-12-04T09:02:51.188453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}